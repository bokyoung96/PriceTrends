import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Tuple, Protocol

import gc
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset as TorchDataset, DataLoader, Subset
from tqdm import tqdm
from numpy.lib.format import open_memmap

from core.loader import DataLoader as CoreLoader

try:
    from .features import make_features
except ImportError:
    from transformer.features import make_features


class ILoader(Protocol):
    def load(self, cols: List[str]) -> Dict[str, pd.DataFrame]:
        ...

class IFeaturizer(Protocol):
    def make(self, frames: Dict[str, pd.DataFrame]) -> 'Panel':
        ...

class IWindowMaker(Protocol):
    def make(self, panel: 'Panel', frames: Dict[str, pd.DataFrame]) -> 'Windows':
        ...


@dataclass(frozen=True)
class Panel:
    values: np.ndarray
    mask: np.ndarray

@dataclass(frozen=True)
class Windows:
    data: np.ndarray
    targets: np.ndarray
    dates: np.ndarray
    assets: np.ndarray

    def save(self, path: Path) -> Path:
        path.parent.mkdir(parents=True, exist_ok=True)
        torch.save({
            "data": self.data,
            "targets": self.targets,
            "dates": self.dates,
            "assets": self.assets,
        }, path, pickle_protocol=4)
        return path

    @staticmethod
    def load(path: Path) -> "Windows":
        # Use weights_only=False because our .pt file contains custom tensors and metadata
        # (saved with pickle protocol 4). This is safe because the file is generated by our own code.
        d = torch.load(path, weights_only=False)
        return Windows(d["data"], d["targets"], d["dates"], d["assets"])

@dataclass(frozen=True)
class Config:
    data_dir: Path = Path(__file__).resolve().parents[1] / "DATA"
    out_path: Optional[Path] = None
    lookback: int = 60
    stride: int = 1
    horizon: int = 20
    min_assets: int = 10
    min_valid: float = 0.95
    norm: Literal["asset", "cross", "none"] = "asset"
    features: Tuple[Any, ...] = ("logreturn", "hlspread", "ocgap", "volumez")
    vol_window: int = 20
    zero_invalid: bool = False
    label_type: Literal["classification", "regression"] = "classification"
    threshold: float = 0.0
    train_years: Optional[List[int]] = None
    test_years: Optional[List[int]] = None

    def __post_init__(self) -> None:
        if self.lookback <= 0: raise ValueError("lookback > 0")
        object.__setattr__(self, "features", tuple(self.features))


class FrameLoader(ILoader):
    def __init__(self, path: Path, exclude: Optional[List[str]] = None):
        self.path = path
        self.loader = CoreLoader(str(path))
        self.exclude = set(self.loader.get_excluded_tickers()) if exclude is None else set(exclude)

    def load(self, cols: List[str]) -> Dict[str, pd.DataFrame]:
        frames = {}
        idx = None
        cols_idx = None

        for name in cols:
            df = self.loader.to_pandas(self.loader.load(name))
            if idx is None: idx = df.index
            if cols_idx is None: cols_idx = df.columns

            df = df.reindex(index=idx, columns=cols_idx)
            if self.exclude:
                df = df.drop(columns=[c for c in self.exclude if c in df.columns], errors="ignore")
            frames[name] = df.astype(np.float64)
        
        return frames

class Featurizer(IFeaturizer):
    def __init__(self, cfg: Config):
        self.cfg = cfg

    def make(self, frames: Dict[str, pd.DataFrame]) -> Panel:
        p = make_features(
            frames,
            features=self.cfg.features,
            norm=self.cfg.norm,
            vol_win=self.cfg.vol_window,
            zero_invalid=self.cfg.zero_invalid,
        )
        return Panel(p.values, p.mask)

class WindowMaker(IWindowMaker):
    def __init__(self, cfg: Config):
        self.cfg = cfg


    def make(self, panel: Panel, frames: Dict[str, pd.DataFrame]) -> Windows:

        
        close = frames["close"]
        dates = close.index.to_numpy()
        assets = close.columns.astype(str).to_numpy()
        
        close_vals = close.to_numpy()
        rets = (close.shift(-self.cfg.horizon) / close) - 1.0
        fut_vals = rets.to_numpy()

        steps, n_assets, n_features = panel.values.shape
        max_start = steps - self.cfg.lookback - self.cfg.horizon
        
        if max_start <= 0: raise ValueError("Not enough steps")

        min_valid = int(np.ceil(self.cfg.lookback * self.cfg.min_valid))
        
        estimated_windows = ((max_start + 1) // self.cfg.stride) * n_assets
        
        temp_dir = Path(__file__).parent / "DATA"
        temp_dir.mkdir(parents=True, exist_ok=True)
        temp_path = temp_dir / "temp_windows.dat"
        
        data_mmap = open_memmap(
            str(temp_path),
            mode='w+',
            dtype=np.float32,
            shape=(estimated_windows, self.cfg.lookback, n_features)
        )
        
        targets, d_list, a_list = [], [], []
        counter = 0
        


        for s in tqdm(range(0, max_start + 1, self.cfg.stride), desc="Creating windows"):
            e = s + self.cfg.lookback
            t_idx = e - 1
            
            mask = panel.mask[s:e]
            counts = mask.sum(axis=0)
            
            fut = fut_vals[t_idx]
            base = close_vals[t_idx]
            fut_ok = np.isfinite(fut) & np.isfinite(base)
            
            ok = (counts >= min_valid) & fut_ok
            idx = np.where(ok)[0]
            
            if len(idx) < self.cfg.min_assets: continue
            
            # NOTE: (n_assets, lookback, n_features)
            slc = panel.values[s:e].transpose(1, 0, 2)
            
            n_valid = len(idx)
            data_mmap[counter:counter+n_valid] = slc[idx]
            
            targets.append(fut[idx])
            d_list.append(np.full(len(idx), dates[t_idx]))
            a_list.append(assets[idx])
            counter += n_valid

        if counter == 0: raise RuntimeError("No windows")

        d = np.array(data_mmap[:counter], dtype=np.float32).copy()
        t = np.concatenate(targets, axis=0).astype(np.float32)
        dt = np.concatenate(d_list, axis=0)
        at = np.concatenate(a_list, axis=0)
        
        del data_mmap
        gc.collect()
        
        if temp_path.exists():
            temp_path.unlink()
        
        if self.cfg.label_type == "classification":
            t = (t > self.cfg.threshold).astype(np.float32)

        return Windows(d, t, dt, at)


class Pipeline:
    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.loader = FrameLoader(cfg.data_dir)
        self.feat = Featurizer(cfg)
        self.win = WindowMaker(cfg)

    def run(self) -> Windows:
        print("Loading...")
        frames = self.loader.load(["open", "high", "low", "close", "volume"])
        
        print("Features...")
        panel = self.feat.make(frames)
        
        print("Windows...")
        wins = self.win.make(panel, frames)
        
        path = self.get_path()
        wins.save(path)
        print(f"Saved to {path}")
        return wins

    def get_path(self) -> Path:
        if self.cfg.out_path: return Path(self.cfg.out_path)
        d = Path(__file__).resolve().parent / "DATA"
        d.mkdir(parents=True, exist_ok=True)
        return d / f"win_lb{self.cfg.lookback}_hz{self.cfg.horizon}.pt"


class StockDataset(TorchDataset):
    def __init__(self, w: Windows):
        self.d = w.data
        self.t = w.targets
        self.dt = w.dates
        self.a = w.assets

    def __len__(self): return len(self.d)

    def __getitem__(self, i):
        return {
            "input": torch.from_numpy(self.d[i]),
            "label": torch.tensor(int(self.t[i]), dtype=torch.long),
            "date": str(self.dt[i]),
            "asset": self.a[i]
        }

def get_loaders(cfg: Config, batch: int = 32, ratio: float = 0.7, workers: int = 0) -> Dict[str, DataLoader]:
    pipe = Pipeline(cfg)
    path = pipe.get_path()
    
    if path.exists():
        print(f"Loading {path}")
        wins = Windows.load(path)
    else:
        wins = pipe.run()
        
    ds = StockDataset(wins)
    
    if cfg.train_years is not None:
        train_years_set = set(cfg.train_years)
        date_years = pd.to_datetime(wins.dates).year.astype(int)
        
        idx = np.arange(len(ds))
        mask = np.isin(date_years, list(train_years_set))
        
        train_ds = Subset(ds, idx[mask])
        val_ds = Subset(ds, idx[~mask])
    else:
        dates = np.unique(wins.dates)
        split = int(len(dates) * ratio)
        train_dates = set(dates[:split])
        
        idx = np.arange(len(ds))
        mask = np.isin(wins.dates, list(train_dates))
        
        train_ds = Subset(ds, idx[mask])
        val_ds = Subset(ds, idx[~mask])
    
    return {
        "train": DataLoader(train_ds, batch_size=batch, shuffle=True, num_workers=workers),
        "validate": DataLoader(val_ds, batch_size=batch, shuffle=False, num_workers=workers)
    }

if __name__ == "__main__":
    Pipeline(Config()).run()
